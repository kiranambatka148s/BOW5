{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4513c9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pylab as pylab\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "import re\n",
    "sentences = \"\"\" The speed of transmission is an important point of difference between the two viruses. Influenza has a shorter median incubation period (the time from infection to appearance of symptoms) and a shorter serial interval (the time between successive cases) than COVID-19 virus. The serial interval for COVID-19 virus is estimated to be 5-6 days, while for influenza virus, the serial interval is 3 days. This means that influenza can spread faster than COVID-19. \n",
    "Further, transmission in the first 3-5 days of illness, or potentially pre-symptomatic transmission –transmission of the virus before the appearance of symptoms – is a major driver of transmission for influenza. In contrast, while we are learning that there are people who can shed COVID-19 virus 24-48 hours prior to symptom onset, at present, this does not appear to be a major driver of transmission. \n",
    "The reproductive number – the number of secondary infections generated from one infected individual – is understood to be between 2 and 2.5 for COVID-19 virus, higher than for influenza. However, estimates for both COVID-19 and influenza viruses are very context and time-specific, making direct comparisons more difficult.  \"\"\"\n",
    "\n",
    "sentences = re.sub('[^A-Za-z0-9]+', ' ', sentences)  #remove the special char\n",
    "sentences = re.sub(r'(?:^| )\\w(?:$| )', ' ', sentences).strip() #remove 1 letter words\n",
    "sentences = sentences.lower()\n",
    "\n",
    "words = sentences.split()\n",
    "vocab = set(words)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "ix_to_word = {i: word for i, word in enumerate(vocab)}\n",
    "\n",
    "data = []\n",
    "for i in range(2, len(words) - 2):\n",
    "    context = [words[i - 2], words[i - 1], words[i + 1], words[i + 2]]\n",
    "    target = words[i]\n",
    "    data.append((context, target))\n",
    "print(data[:5])\n",
    "\n",
    "embeddings = np.random.random_sample((vocab_size, 10))\n",
    "\n",
    "def linear(m, theta):\n",
    "    w = theta\n",
    "    return m.dot(w)\n",
    "\n",
    "def log_softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return np.log(e_x / e_x.sum())\n",
    "def NLLLoss(logs, targets):\n",
    "    out = logs[range(len(targets)), targets]\n",
    "    return -out.sum()/len(out)\n",
    "def log_softmax_crossentropy_with_logits(logits,target):\n",
    "    out = np.zeros_like(logits)\n",
    "    out[np.arange(len(logits)),target] = 1\n",
    "    softmax = np.exp(logits) / np.exp(logits).sum(axis=-1,keepdims=True)\n",
    "    return (- out + softmax) / logits.shape[0]\n",
    "\n",
    "def forward(context_idxs, theta):\n",
    "    m = embeddings[context_idxs].reshape(1, -1)\n",
    "    n = linear(m, theta)\n",
    "    o = log_softmax(n)\n",
    "    return m, n, o\n",
    "\n",
    "def backward(preds, theta, target_idxs):\n",
    "    m, n, o = preds\n",
    "    dlog = log_softmax_crossentropy_with_logits(n, target_idxs)\n",
    "    dw = m.T.dot(dlog)\n",
    "    return dw\n",
    "\n",
    "def optimize(theta, grad, lr=0.03): \n",
    "    theta -= grad * lr\n",
    "    return theta\n",
    "\n",
    "# c. Train model\n",
    "theta = np.random.uniform(-1, 1, (2 * 2 * 10, vocab_size))\n",
    "epoch_losses = {}\n",
    "for epoch in range(80):\n",
    "    losses = []\n",
    "    for context, target in data:\n",
    "        context_idxs = np.array([word_to_ix[w] for w in context])\n",
    "        preds = forward(context_idxs, theta)\n",
    "        target_idxs = np.array([word_to_ix[target]])\n",
    "        loss = NLLLoss(preds[-1], target_idxs)\n",
    "        losses.append(loss)\n",
    "        grad = backward(preds, theta, target_idxs)\n",
    "        theta = optimize(theta, grad, lr=0.03)\n",
    "    epoch_losses[epoch] = losses\n",
    "    \n",
    "ix = np.arange(0,80)\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Epoch/Losses', fontsize=20)\n",
    "plt.plot(ix,[epoch_losses[i][0] for i in ix])\n",
    "plt.xlabel('Epochs', fontsize=12)\n",
    "plt.ylabel('Losses', fontsize=12)\n",
    "\n",
    "def predict(words):\n",
    "    context_idxs = np.array([word_to_ix[w] for w in words])\n",
    "    preds = forward(context_idxs, theta)\n",
    "    word = ix_to_word[np.argmax(preds[-1])]\n",
    "    return word\n",
    "# (['we', 'are', 'to', 'study'], 'about')\n",
    "#predict(['we', 'are', 'about', 'study'])\n",
    "\n",
    "def accuracy():\n",
    "    wrong = 0\n",
    "    for context, target in data:\n",
    "        if(predict(context) != target):\n",
    "            wrong += 1\n",
    "    return (1 - (wrong / len(data)))\n",
    "accuracy()\n",
    "\n",
    "# d. Output\n",
    "predict(['transmission', 'is', 'important', 'point'])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
